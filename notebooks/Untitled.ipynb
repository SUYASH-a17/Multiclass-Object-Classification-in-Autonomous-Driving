{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b33379f5-9d6d-4cd2-8858-bf1c7962a35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f220c562-f389-47ad-b870-afe5ccb11201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Check TensorFlow and Keras Versions\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Check TensorFlow and Keras versions\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"Keras Version:\", tf.keras.__version__)\n",
    "\n",
    "# If you need to install specific versions, use:\n",
    "# pip install tensorflow==2.12 keras==2.12\n",
    "\n",
    "# Step 2: Save and Load Models Using TensorFlow SavedModel Format\n",
    "\n",
    "# Assuming you already have a trained MobileNetV2 model\n",
    "# Save the model in SavedModel format\n",
    "MODEL_SAVE_PATH = 'fine_tuned_mobilenetv2'\n",
    "mobilenet_model.save(MODEL_SAVE_PATH)\n",
    "print(f\"Model saved to {MODEL_SAVE_PATH} in TensorFlow SavedModel format.\")\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model(MODEL_SAVE_PATH)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Step 3: Handle Custom Layers (if Any)\n",
    "# Handle custom layers if your model includes them (e.g., BatchNormalization)\n",
    "custom_objects = {'BatchNormalization': BatchNormalization}\n",
    "model_with_custom = tf.keras.models.load_model(MODEL_SAVE_PATH, custom_objects=custom_objects)\n",
    "print(\"Model with custom layers loaded successfully.\")\n",
    "\n",
    "# Step 4: Convert the Model to TensorFlow Lite\n",
    "# Convert the model to TensorFlow Lite format for deployment\n",
    "TFLITE_SAVE_PATH = 'fine_tuned_mobilenetv2.tflite'\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_SAVE_PATH)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open(TFLITE_SAVE_PATH, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "print(f\"Model converted to TensorFlow Lite and saved as {TFLITE_SAVE_PATH}.\")\n",
    "\n",
    "# Step 5: Debugging and Testing\n",
    "# Load the TensorFlow Lite model and perform a test prediction\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_SAVE_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Test input data (replace with real input for your model)\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Assuming input shape is (1, 224, 224, 3) for MobileNetV2\n",
    "import numpy as np\n",
    "test_input = np.random.random_sample(input_details[0]['shape']).astype(np.float32)\n",
    "\n",
    "# Perform inference\n",
    "interpreter.set_tensor(input_details[0]['index'], test_input)\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(\"Test Prediction Output:\", output_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
